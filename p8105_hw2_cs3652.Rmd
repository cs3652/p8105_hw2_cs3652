---
title: "HW2_cs3652"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## GitHub Documents

This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

```{r}
library(tidyverse)
```

##Problem 1

```{r, message = FALSE}
subway_data = read_csv(file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", col_types = "cccddcccccccccccccccccccccccddcc") %>%
  janitor::clean_names() %>% 
  ##cleaning names so that they are lowercase snake
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  ##selecting relevant variables
  mutate(entry = recode(entry, 'YES' = TRUE, 'NO' = FALSE))
  ##recoding to logical variable 
```
  
This dataset has many variables that describe the MTA service at New York City subway stations. It includes the routes that each station serves, as well as entry/exit points, and ada compliance, among many other things. Thus far, the data cleaning steps that I have taken so far are: cleaning the names so that they are easy to read and are lower case snake, selecting the columns (variables) that are relevant to our analysis, and mutating the entry variable so that it is a logical variable. The dimension of the resulting dataset is `r dim(subway_data)` (that is 1868 rows x 19 columns). These data are not entirely tidy since the route variables can be futher simplified into more usable data based on the analyses that we want to conduct. 

```{r, message = FALSE}
subway_data %>% 
  distinct(line, station_name) 
  ##finding the distinct number of stations 
select(subway_data, line, station_name, ada) %>% 
  filter(ada == "TRUE") %>% 
  distinct(line, station_name, ada)
  ##finding the number of ADA compliant stations 
nrow(filter(subway_data, entry == TRUE, vending == "NO")) / nrow(filter(subway_data, vending == "NO"))
  ##finding the proportion of station entrances without vending allowing entrance. 
```

According to this dataset there are `r count(distinct(subway_data, line, station_name))` distinct stations. There are `r nrow(filter(distinct(subway_data, line, station_name, ada), ada == TRUE))` stations that are ADA compliant. Furthermore, the proportion of stations extrances/exits, without vending, that allow entrance is `r nrow(filter(subway_data, entry == TRUE, vending == "NO"))/nrow(filter(subway_data, vending == "NO"))` or 37.7%.  

```{r, message = FALSE}
subway_data_tidy = (gather(subway_data, key = route_number, value = route, route1:route11)) 
#creating a new dataframe where route number information is simplified
```

```{r, message = FALSE}
nrow(filter(distinct(subway_data_tidy, line, station_name, route), route == "A"))
##finding number of stations on the A line
nrow(filter(distinct(subway_data_tidy, line, station_name, route, ada), route == "A", ada == TRUE))
##finding the number of ADA compliant stations on the A line
```

There are `r nrow(filter(distinct(subway_data_tidy, line, station_name, route), route == "A"))` stations on the A line. Only `r nrow(filter(distinct(subway_data_tidy, line, station_name, route, ada), route == "A", ada == TRUE))` of the 60 stations are ADA compliant.

##Problem 2

```{r}
trashwheel_data = readxl::read_excel("./HealthyHarborWaterWheelTotals2018-7-28.xlsx", "Mr. Trash Wheel", range = cellranger::cell_cols("A:N")) %>% 
  ##Reading in data from excel sheet using a specific sheet and cell range
  janitor::clean_names() %>% 
  ##Cleaning names to lowercase snake
  filter(!is.na(dumpster)) %>% 
  ##removing rows without dumpster data
  mutate(sports_balls = as.integer(round(sports_balls)))
  ##rounding sport_balls to nearest integer and mutating it to an integer variable

trashwheel_data %>% 
  filter(year == "2016") %>% 
  summarize(median(sports_balls))
```

The Mr. Trash Wheel dataset has `r nrow(trashwheel_data)` observations. In 2016, the median number of sports balls in the trash was `r trashwheel_data %>% filter(year == "2016") %>% summarise(median(sports_balls))`.

```{r}
precipitation16_data = readxl::read_excel("./HealthyHarborWaterWheelTotals2018-7-28.xlsx", "2016 Precipitation", range = cellranger::cell_cols("A:B")) %>% 
  ##reading in precipitation data from 2016 specifying cell range
  janitor::clean_names() %>% 
  ##cleaning names into lowercase snake
  filter(!is.na(precipitation_in) & precipitation_in != ("Month")) %>% 
  ##removing rows without precipitation
  add_column(year = "2016") %>% 
  ##creating variable called year
  rename(month = precipitation_in) %>% 
  ##renaming precipitation_in
  rename(rainfall_in = x_1) %>% 
  ##renaming x_1
  mutate(rainfall_in = as.numeric(rainfall_in))

precipitation17_data = readxl::read_excel("./HealthyHarborWaterWheelTotals2018-7-28.xlsx", "2017 Precipitation", range = cellranger::cell_cols("A:B")) %>% 
  ##reading in precipitation data from 2017 specifying cell range
  janitor::clean_names() %>% 
  ##cleaning names into lowercase snake
  filter(!is.na(precipitation_in) & !is.na(x_1) & precipitation_in != ("Month")) %>% 
  ##removing rows without precipitation
  add_column(year = "2017") %>% 
  ##creating variable called year
  rename(month = precipitation_in) %>%
  ##renaming precipitation_in
  rename(rainfall_in = x_1) %>% 
  ##renaming x_1
  mutate(rainfall_in = as.numeric(rainfall_in))

precipitation17_data %>% 
  select(rainfall_in) %>% 
  sum()
  ##finding total precipitation in 2017

precipitation_new_data = bind_rows(precipitation16_data, precipitation17_data) %>% 
  ##combining datasets
  mutate(month = month.name[as.integer(month)])
  ##converting month to a character variable
```

In the 2016 dataset there are `r nrow(precipitation16_data)` observations and in the 2017 dataset there are `r nrow(precipitation17_data)` observations. Key variables include the month, year and rainfall_in variables. Using these variables we can clearly see the amount of precipitation in each month of 2016 and 2017. The total precipitation for 2017 was `r precipitation17_data %>% select(rainfall_in) %>% sum()` inches.

##Problem 3

```{r}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data("brfss_smart2010")

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  ##cleaning names into lowercase snake
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -sample_size, -confidence_limit_low:-geo_location) %>% 
  ##excluding variables
  spread(key = response, value = data_value) %>% 
  ##restructuring data so that values for response are variables that present the proportion of subjects with each response 
  janitor::clean_names()

brfss_data %>% 
  mutate(proportion_excellent_verygood = (excellent + very_good) / 100) %>% 
  ##creating new variable showing proportion of responses that are excellent or very good
  filter(year == "2002", !is.na(excellent))

brfss_data %>% 
  distinct(locationabbr) %>% 
  count()
  ##finding the number of unique locations

brfss_data %>% 
  count(locationabbr) %>% 
  arrange(desc(n)) %>% 
  select(locationabbr) %>% 
  head(1)
```

In this dataset, there are `r count(distinct(brfss_data, locationdesc))` unique locations. Additionally, all states are represented. I know this by counting the number of values for locationabbr (`r brfss_data %>% distinct(locationabbr) %>% count()`), which includes the 50 states and Washington D.C. The most observed state was `r brfss_data %>% count(locationabbr) %>% arrange(desc(n)) %>% select(locationabbr) %>% head(1)`. The median of the "Excellent" response value was `r brfss_data %>% filter(year == "2002", !is.na(excellent)) %>% summarize(median(excellent))`, in 2002.

```{r}
brfss_data %>% 
  filter(year == "2002", !is.na(excellent)) %>% 
  ggplot(aes(x = excellent)) + geom_histogram(binwidth = 0.5) +
  labs(
    title = "Excellent Responses Histogram",
    x = "Excellent Response Values",
    y = "Count"
  ) +
  ggthemes::theme_economist()
  ##creating histogram reflecting excellent response values in 2002
```

```{r}
brfss_data %>% 
  filter(locationdesc == "NY - Queens County" | locationdesc == "NY - New York County") %>% 
  ggplot(aes(x = year, y = excellent, color = locationdesc)) + geom_point() +
  labs(
    title = "Proportion of Excellent Responses in New York and Queens over Time",
    x = "Year",
    y = "Proportion of Excellent Responses"
  ) +
  ggthemes::theme_economist()
  ##creating a scatterplot showing proportion of excellent response values in New York County and Queens County in each year from 2002 to 2010
```
